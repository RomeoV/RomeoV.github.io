<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2024-01-14 Sun 17:35 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Dictionary Learning for Disentangling Concepts in Vision Transformers.</title>
<meta name="author" content="Romeo Valentin" />
<meta name="generator" content="Org Mode" />
<style>
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">Dictionary Learning for Disentangling Concepts in Vision Transformers.</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org607b06b">1. Introduction.</a>
<ul>
<li><a href="#orgecfa19d">1.1. Small number of &ldquo;active&rdquo; concepts.</a></li>
<li><a href="#org84a13ae">1.2. Natural language encoding.</a></li>
</ul>
</li>
<li><a href="#orgdca7e37">2. Dictionary Learning with the K-SVD algorithm. </a>
<ul>
<li><a href="#org804be58">2.1. The algorithm, and theoretical considerations.</a>
<ul>
<li><a href="#org1d37f03">A note on convergence.</a></li>
</ul>
</li>
<li><a href="#orgb4d258d">2.2. KSVD.jl : A highly optimized K-SVD implementation.</a>
<ul>
<li><a href="#orgfa0b6fd">Over-the-thumb estimation of computational requirements for large dataset.</a></li>
</ul>
</li>
<li><a href="#org61b33aa">Dictionary Learning vs Sparse Autoencoder. </a></li>
</ul>
</li>
<li><a href="#org1dfb78e">3. Creating the token mapping. </a>
<ul>
<li><a href="#orgd7cfd42">3.1. Correlating word occurrences with sparse assignments. </a>
<ul>
<li><a href="#orga143333">3.1.1. Limitations:</a></li>
</ul>
</li>
<li><a href="#orgee9fae0">3.2. Minimizing semantic distance between image caption and list of descriptive tokens.</a>
<ul>
<li><a href="#orgf0ffef0">3.2.1. Finding tokens by optimizing in the token embedding space.</a></li>
<li><a href="#orgda42e38">3.2.2. Finding token embeddings through greedy search.</a></li>
</ul>
</li>
<li><a href="#org985373d">3.3. Dictionary learning on a combined (caption-embedding \(\cup\) image-embedding) dataset.</a></li>
</ul>
</li>
<li><a href="#org7dc4517">4. Experimental results</a></li>
<li><a href="#org9a10f53">5. Conclusion and current limitations.</a></li>
<li><a href="#org696ba02">6. Bibliography.</a></li>
<li><a href="#orgbeb0049">7. Appendix</a>
<ul>
<li><a href="#org6d57154">7.1. Correlation patterns </a></li>
</ul>
</li>
</ul>
</div>
</div>
<p>
<b>Abstract:</b> Vision Transformers produce high-quality embeddings useful for many downstream tasks &#x2013; however both their embeddings and inner workings are typically difficult to interpret.
However, by disentangling the conceptual components of the data we can hope to gain additional insight; however finding disentangled human-aligned has been shown to be difficult in the past.
In this work we propose using the k-SVD algorithm together with captioned image datasets to tackle this problem.
We present a new highly optimized implementation of the k-SVD algorithm that scales to large datasets with millions of samples and can be scaled to many compute nodes.
Then we discuss three ways how we can map the resulting dictionary elements from the k-SVD algorithm to natural language word tokens in a &ldquo;faithful&rdquo; way.
</p>

<p>
This work is still in progress, and reported here are some of the results obtained while working on this in Fall 2023.
</p>
<div id="outline-container-org607b06b" class="outline-2">
<h2 id="org607b06b"><span class="section-number-2">1.</span> Introduction.</h2>
<div class="outline-text-2" id="text-1">
<p>
The Transformer architecture has been successfully deployed for a variety of applications.
However, as with other machine learning methods, its reliability is not guaranteed.
Reliability can for example be measured as a combination of in-distribution performance, performance under distributional shift, out-of-distribution detection and/or performance, uncertainty calibration, and robustness to adversarially chosen input perturbations.
</p>

<p>
We hypothesize that for robustness and generalizability, the model must learn a faithful representation of the world, which goes beyond mere statistical inference.
</p>

<p>
Humans similarly have a model of the world, and although it is generally not easy to write down, we have developed natural language as a way to communicate information between each other.
It is therefore natural to wonder whether the models potentially emerging in large Transformer networks are or can be aligned with a human model of the world.
</p>

<p>
Let us consider the special case of Vision Transformers.
Typically, models are trained to produce a sequence of high-dimensional embeddings, which can serve as inputs to downstream tasks like classification.
It is therefore clear that these embeddings have a variety of information stored in them.
</p>

<p>
The goal is now try to &ldquo;disentangle&rdquo; these embeddings in order to provide a interpretable/human-aligned/monosemantic view of the embeddings, which stay faithful to the original content of the embedding (but might sacrifice some of the detailed information).
This goal of &ldquo;disentanglement&rdquo; from a &ldquo;block of data&rdquo; goes back to Independent Component Analysis (ICA), and was picked up again in the context of machine learning (see e.g. beta-VAE (<a href="#citeproc_bib_item_4">Higgins et al. 2017</a>)).
&ldquo;Disentanglement&rdquo; here is defined as having a set of components that are both conditionally independent and align with a notion of human interpretability.
However, an important result in 2020 proved that without further assumptions there is an infinite set of components which satisfy the conditional independence property, so requiring this alone is not enough to get interpretable components for free (<a href="#citeproc_bib_item_5">Locatello et al. 2019</a>).
</p>

<p>
However, in this work we propose two extra conditions with which we aim to overcome the theoretical concerns:
</p>
<ol class="org-ol">
<li>small number of &ldquo;active&rdquo; concepts, and</li>
<li>encodable using natural language.</li>
</ol>

<p>
In the next sections we first discuss some theoretical details for both conditions, and then describe how we actually implement each condition, i.e through dictionary learning/the k-SVD algorithm (Section <a href="#orga6bc321">2</a>) and token assignment (Section <a href="#org6f95a05">3</a>).
</p>
</div>
<div id="outline-container-orgecfa19d" class="outline-3">
<h3 id="orgecfa19d"><span class="section-number-3">1.1.</span> Small number of &ldquo;active&rdquo; concepts.</h3>
<div class="outline-text-3" id="text-1-1">
<p>
The first condition is motivated easily.
It turns out to be easy to project a dataset onto a new basis of orthogonal vectors, i.e. &ldquo;statistically independent concepts&rdquo;, so that each datapoint is a sum of these orthogonal vectors &#x2013; this is indeed just the singular value decomposition (SVD), i.e. \(Y = U \Sigma V^\top\) where each column of \(Y\) is a datapoint, and each column of \(U\) is a &ldquo;concept vector&rdquo;.
</p>

<p>
However, in this case each datapoint is constructed of an &ldquo;infinite sum&rdquo; of concept vectors, whereas we believe that any data sample only has a relatively small number of active concepts.
Further, the SVD is designed to find something like &ldquo;Principal Components&rdquo;, which in our language would translate to abstract concepts that are represented in the majority of datapoints.
However, we want neither of these.
Instead, we would like to represent each datapoint \(y\) for example as a sum \(\sum_{j \in \mathcal{J}} \vec{d}_j \cdot \lambda_j\) where \(|\mathcal{J}|\) is small.
In other words, we try to write our data matrix \(Y = [y_1 \dots y_{|Y|}]\) as a product of dictionary vectors \(D = [d_1 \dots d_{|D|}]\) and a sparse assignment matrix \(X\) such that \(Y \approx D X\).<sup><a id="fnr.1" class="footref" href="#fn.1" role="doc-backlink">1</a></sup>
</p>

<p>
It turns out that this problem has been studied previously, and is known as &ldquo;Dictionary Learning&rdquo;.
A popular algorithm for dictionary learning is the &ldquo;K-SVD&rdquo; algorithm (<a href="#citeproc_bib_item_1">Aharon, Elad, and Bruckstein 2006</a>), which essentially alternates between updating the set of dictionary vectors and updating the assignment matrix.
We will go into more details in Section <a href="#orga6bc321">2</a>.
</p>

<p>
Recently, it has also been proposed to use a Sparse Autoencoder, e.g. as presented in (<a href="#citeproc_bib_item_3">Bricken et al. 2023</a>).
We briefly compare the approaches <a href="#org85ae4e3">below</a>.
</p>
</div>
</div>
<div id="outline-container-org84a13ae" class="outline-3">
<h3 id="org84a13ae"><span class="section-number-3">1.2.</span> Natural language encoding.</h3>
<div class="outline-text-3" id="text-1-2">
<ul class="org-ul">
<li>The additional sparsity constraint is enough to overcome the theoretical problems brought up by Locatello et al. (<a href="#citeproc_bib_item_5">2019</a>).</li>
<li>However, it remains unclear whether this is enough for extracting dictionary or concept vectors with the kind of human-aligned interpretability that we are looking for.</li>

<li>Indeed, even if they were sufficiently aligned, it is not clear how we can map the concept vectors to natural language representations, or how we can test whether a language-to-embedding mapping is &ldquo;faithful&rdquo; or correct in some sense.</li>
<li>However, we believe that the recently gained availabily to large language models (LLMs) gives us a way to overcome these problems.</li>
</ul>


<blockquote>
<p>
<i><b>Proposition 1:</b> Consider the setup presented in Fig. <a href="#orgdc2f951">1</a>, and consider that the disentangling method works well in the sense described in the previous section.
Then, we consider a token mapping &ldquo;faithful&rdquo; if the diagram in Fig. <a href="#orgdc2f951">1</a> commutes.</i>
</p>
</blockquote>


<div id="orgdc2f951" class="figure">
<p><img src="./tikz-pictures/cd-diagram/main.png" alt="main.png" width="600" />
</p>
<p><span class="figure-number">Figure 1: </span><a href="https://en.wikipedia.org/wiki/Commutative_diagram">Commutative diagram</a> showing the flow of semantic information. Given captioned images, a ViT, an LLM, and a disentangling method, we can therefore create a token mapping.</p>
</div>

<p>
Being able to &ldquo;score&rdquo; a given token mapping now may make it possible to actually construct the token mapping.
We go into more detail on the approaches in Section <a href="#org6f95a05">3</a>.
</p>
</div>
</div>
</div>
<div id="outline-container-orgdca7e37" class="outline-2">
<h2 id="orgdca7e37"><span class="section-number-2">2.</span> Dictionary Learning with the K-SVD algorithm. <a id="orga6bc321"></a></h2>
<div class="outline-text-2" id="text-2">
<p>
In this section we first discuss some theoretical properties and considerations of the K-SVD algorithm, and then discuss the contributions we have made and published in the <a href="https://github.com/RomeoV/KSVD.jl">KSVD.jl</a> package.
Then we will briefly discuss the K-SVD algorithm versus using a Sparse Autoencoder.
</p>
</div>
<div id="outline-container-org804be58" class="outline-3">
<h3 id="org804be58"><span class="section-number-3">2.1.</span> The algorithm, and theoretical considerations.</h3>
<div class="outline-text-3" id="text-2-1">
<p>
The K-SVD algorithm solves the problem of dictionary learning outlined above, namely finding a matrix decomposition \(Y \approx D X\) where \(\mathrm{size}(D, 2) > \mathrm{size}(D, 1)\) (i.e. an &ldquo;overcomplete&rdquo; dictionary), and \(X\) is sparse.
It can be understood as a generalization to the k-means algorithm, but allowing any datapoint to be associated with <i>multiple centroids</i>.
</p>

<p>
Typically, iterative dictionary learning algorithms alternate between two steps:
</p>
<dl class="org-dl">
<dt>Sparse coding:</dt><dd>Given a fixed dictionary, for each data sample find a small subset of dictionary vectors and factors such that \(y_i \approx \sum_{j \in \mathcal{J}} x_j \cdot d_j\).
Note that this is a non-convex problem (and indeed NP-hard), and typically heuristics like greedy search are used. Typically used algorithms are (Orthogonal) Matching Pursuit, Basis Pursuit, or FOCUSS.
In our implementation, we use Matching Pursuit.</dd>
<dt>Dictionary update:</dt><dd>Given a coding matrix \(X\), we can again update the dictionary elements. Several approaches exist &#x2013; for example, a simple gradient descent based approach can be used to optimize \(\min_D \|Y - DX\|\), where \(X\) stays fixed.
However, Aharon, Elad, and Bruckstein (<a href="#citeproc_bib_item_1">2006</a>) propose updating each dictionary element independently by considering all the data points that are &ldquo;using&rdquo; a given dictionary element and then replacing the dictionary element with the dominant singular vector of the these datapoints.
Note that this simultaneously updates the values in \(X\), which most other algorithms do not.</dd>
</dl>


<div id="org6e6e5da" class="figure">
<p><img src="./tikz-pictures/ksvd-overview/main.png" alt="main.png" width="500" />
</p>
<p><span class="figure-number">Figure 2: </span>An overview of the K-SVD algorithm.</p>
</div>

<p>
This algorithm has exceptionally few hyperparameters &#x2013; indeed, it is sufficient to choose a number of dictionary elements, and a cutoff threshold for sparse coding.<sup><a id="fnr.2" class="footref" href="#fn.2" role="doc-backlink">2</a></sup> A mere two hyperparameters!
Further, utilizing the (truncated) svd algorithm typically leads to fast convergence and good computational efficiency, since the svd is theoretically &ldquo;optimal&rdquo; in some sense, and heavily optimized.
</p>
</div>
<div id="outline-container-org1d37f03" class="outline-4">
<h4 id="org1d37f03">A note on convergence.</h4>
<div class="outline-text-4" id="text-org1d37f03">
<p>
It is noteworthy that K-SVD has a convergence guarantee <i>only to a local minimum</i>, and only if the sparse coding step <i>is solved optimally</i>.
However, as is somewhat typical for machine learning, we have some hope that for large enough datasets the loss function becomes practically convex, so that the algorithm converges to a global minimum.
</p>
</div>
</div>
</div>
<div id="outline-container-orgb4d258d" class="outline-3">
<h3 id="orgb4d258d"><span class="section-number-3">2.2.</span> KSVD.jl : A highly optimized K-SVD implementation.</h3>
<div class="outline-text-3" id="text-2-2">
<p>
<i>Note: While we believe the timings described in this section to be roughly correct, we have to run additional tests to make sure we also achieve the same convergence.</i>
</p>

<p>
Bricken et al. (<a href="#citeproc_bib_item_3">2023</a>) mention the K-SVD algorithm, however deem it computationally infeasible to apply to large datasets with millions (or billions) of samples.
And indeed, current implementations seem not up to the task; the implementation available as <code>sklearn.decompositions.MiniBatchDictionaryLearning</code>, which extensively leverages <code>numpy</code> and <code>joblib</code>, takes over 3 minutes for ten iterations on a dataset with ten thousand elements (despite multi-threading).
</p>

<p>
For this reason, we present <a href="https://github.com/RomeoV/KSVD.jl"><code>KSVD.jl</code></a>, an implementation of the K-SVD algorithm in the Julia programming language (<a href="#citeproc_bib_item_2">Bezanson et al. 2017</a>).
This implementation outperforms sklearn&rsquo;s implementation by about \(50\times\) when computing the same problem, can gain an additional \(2\times\) by reducing the precision from <code>Float64</code> to <code>Float32</code>, and can be scaled across many compute nodes with almost linear speedup improvements<sup><a id="fnr.3" class="footref" href="#fn.3" role="doc-backlink">3</a></sup>.
That means if, for example, eight compute nodes are available, we can expect a speedup of \((50 \cdot 2 \cdot 8)\times = 800\times\) for moderate to large datasets.
Further, <code>KSVD.jl</code> also employs several algorithmic modifications that, to the author&rsquo;s knowledge, lead to faster convergence given the same number of compute operations.<sup><a id="fnr.4" class="footref" href="#fn.4" role="doc-backlink">4</a></sup>
</p>

<p>
This speedup has been achieved through extensive benchmarking and optimization of the code, including
</p>
<ul class="org-ul">
<li>careful adjustments to the execution order and small algorithmic adjustments,</li>
<li>single-core optimizations like aggressive buffer preallocations, exploiting cache locality, improving the memory layout, and reducing memory movements,</li>
<li>careful multi-threading using small batch updates with frequent cross-communication implemented with Julia&rsquo;s efficient task scheduling,</li>
<li>a custom multi-threaded dense-sparse matrix multiplication implementation (<a href="https://github.com/RomeoV/ThreadedDenseSparseMul.jl"><code>ThreadedDenseSparseMul.jl</code></a>),</li>
<li>pipelined GPU-offloading for large matrix multiplications (currently unused in the fastest version),</li>
<li>a custom distributed executor allowing to spread the computation over many compute nodes.</li>
</ul>
</div>
<div id="outline-container-orgfa0b6fd" class="outline-4">
<h4 id="orgfa0b6fd">Over-the-thumb estimation of computational requirements for large dataset.</h4>
<div class="outline-text-4" id="text-orgfa0b6fd">
<p>
To illustrate the (theoretical) execution times on a large dataset, let us estimate the time to compute 10 and 100 iterations of the K-SVD algorithm on embeddings from the OpenCLIP dataset, which has 400 million samples.
We will consider having a cluster with 8 nodes and 64 cores each, and compute in <code>Float32</code> precision.
</p>

<p>
As a datapoint, let&rsquo;s consider measurements from my 16 thread (8 core) Intel i7 mobile processor, which achieves 10 iterations on 800&rsquo;000 samples in about 120 seconds.
Using the setup above, we have about 32 times more compute resources and a 500 times larger problem, which yields an estimated runtime of \(120\text{sec} \cdot \frac{500}{32} = 1875\text{sec} \approx 0.5\text{h}\), and similarly \(5\text{h}\) for 100 iterations etc.
</p>
</div>
</div>
</div>
<div id="outline-container-org61b33aa" class="outline-3">
<h3 id="org61b33aa">Dictionary Learning vs Sparse Autoencoder. <a id="org85ae4e3"></a></h3>
<div class="outline-text-3" id="text-org61b33aa">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Method</th>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-left">k-svd</th>
<th scope="col" class="org-left">gradient</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">find sparse assignments via&#x2026;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">greedy search</td>
<td class="org-left">\(L_1\) loss</td>
</tr>

<tr>
<td class="org-left">number of hyperparameters</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">1/2</td>
<td class="org-left">many</td>
</tr>

<tr>
<td class="org-left">&ldquo;one&rdquo; way to do it</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">yes</td>
<td class="org-left">no</td>
</tr>

<tr>
<td class="org-left">can get stuck in local minima?</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">yes (?)</td>
<td class="org-left">yes (?)</td>
</tr>

<tr>
<td class="org-left">runtime?</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">Comparison tbd&#x2026;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">quality of result?</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">Comparison tbd&#x2026;</td>
<td class="org-left">&#xa0;</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div id="outline-container-org1dfb78e" class="outline-2">
<h2 id="org1dfb78e"><span class="section-number-2">3.</span> Creating the token mapping. <a id="org6f95a05"></a></h2>
<div class="outline-text-2" id="text-3">
<p>
In the previous section, we saw how we can extract &ldquo;concept&rdquo; or &ldquo;dictionary&rdquo; vectors that explain the embeddings generated by a ViT.
Crucially, we assume for now that each dictionary vector can be mapped to a human concept.
This is a strong hypothesis, as it requires (i) that the introduction of the sparsity constraint overcomes the concerns mentioned in Locatello et al. (<a href="#citeproc_bib_item_5">2019</a>), and that (ii) the k-SVD algorithm does a sufficiently good job at solving the problem.
</p>

<p>
However, even in this case it is not trivial to actually find a way to assign the human concept.
We propose several ways how this can be achieved nonetheless.
All ways rely on having <i>captioned images</i>, i.e. dataset of pairs connecting a natural-language description and an associated image.
In particular, we work with the <i>sbucaptions</i> dataset (<a href="#citeproc_bib_item_6">Ordonez, Kulkarni, and Berg 2011</a>), which contains about 800 thousand captioned images, see e.g. <a href="https://vislang.ai/sbu-explorer">https://vislang.ai/sbu-explorer</a>.
Other datasets include coco (600k), conceptual captions (3M and 12M), laion datasets (400M and 5B), and others.
</p>

<p>
The approach for all the following methods is always similar:
</p>
<ol class="org-ol">
<li><i>Only</i> the images are fed to a ViT model, and a single embedding vector per image is constructed (usually 768 or 1536 dimensional), yielding an embedding matrix like \(Y \in \mathbb{R}^{768 \times n}\).</li>
<li>Dictionary learning (e.g. with the k-SVD algorithm) is used to rewrite \(Y\) as \(Y \approx D X\) with \(X\) sparse and \(D\) overcomplete.</li>
<li>The captions are processed somehow, e.g. by computing an embedding or checking them for certain concept nouns. The result constitutes a kind of label.</li>
<li>These labels are used to assign concepts to each dictionary element in \(D\).</li>
</ol>
<p>
Notice that these steps roughly model the diagram in Fig. <a href="#orgdc2f951">1</a>.
</p>

<p>
We will now go over some concrete examples over how this can be achieved.
We denote approaches that have been implemented in some form of proof-of-concept, and which have not.
</p>
</div>
<div id="outline-container-orgd7cfd42" class="outline-3">
<h3 id="orgd7cfd42"><span class="section-number-3">3.1.</span> Correlating word occurrences with sparse assignments. <a id="org09f021c"></a></h3>
<div class="outline-text-3" id="text-3-1">
<p>
We first present an approach not reliant on LLMs at all, and instead works by correlating \(X\) with word occurrences in the captions.
We start by considering a list of English words, e.g. nouns <a href="https://github.com/hugsy/stuff/blob/main/random-word/english-nouns.txt">from here</a>.
For each caption, we then check if the noun is contained in the caption, yielding a occurrence matrix \(O \in \left\{ 0, 1 \right\}^{n_{\rm words} \times n_{\rm captions}}\).
</p>

<p>
Recall now that we have \(X \in \left\{ 0, 1 \right\}^{n_{\rm dictionary} \times n_{\rm words}}\) and let \(X_{01} \in \left\{ 0,1 \right\}^{n_{\rm dictionary}\times n_{\rm captions}}\) be the &ldquo;indicator&rdquo; matrix of \(X\), i.e. \(X_{01}[i,j] := (X[i,j] \neq 0)\).
Then, we can compute the correlation between \(O\) and \(X_{01}\); informally, if a certain dictionary element has been selected for a set of images, and all the images&rsquo; captions contain the word &ldquo;dog&rdquo;, then the dictionary concept is at least statistically related to the concept &ldquo;dog&rdquo;.
</p>

<p>
More precisely, let \(C = {cor}(O, X_{01}) \in \mathbb{R}^{n_{\rm words} \times n_{dictionary}}\) be this correlation, reduced along the images/captions.
Then, for each dictionary element we can assign e.g. the top-1 or top-3 most correlating words by simply finding the maximum correlation column-wise.
Some more discussed on correlation magnitudes is in Section <a href="#org92f9ae8">7.1</a> (Appendix).
</p>

<div class="org-src-container">
<label class="org-src-name"><span class="listing-number">Listing 1: </span>Finding word tokens for dictionary elements by correlating \(X\) with word occurrences in image captions.</label><pre class="src src-julia" id="org12bb9a7">(D, X) = deseralize(<span style="color: #79740e;">"sbu_captions_embeddings_ksvd.jls"</span>)
X_01 = (X .!= <span style="color: #8f3f71; font-weight: bold;">0</span>)

words = readlines(<span style="color: #79740e;">"english-nouns.txt"</span>)
captions = map(lowercase, readlines(<span style="color: #79740e;">"captions.txt"</span>))
O = [occursin(word, caption) <span style="color: #9d0006;">for</span> word <span style="color: #9d0006;">in</span> words,
                                 caption in captions]
n = size(X_01, <span style="color: #8f3f71; font-weight: bold;">2</span>); <span style="color: #076678; font-weight: bold;">@assert</span> size(X_01, <span style="color: #8f3f71; font-weight: bold;">2</span>) <span style="color: #8f3f71;">==</span> size(O, <span style="color: #8f3f71; font-weight: bold;">2</span>)

C = cor(O, X_01; dims=<span style="color: #8f3f71; font-weight: bold;">2</span>)

word_indices_per_dict_element = [
    sortperm(col; rev=<span style="color: #8f3f71;">true</span>)[<span style="color: #8f3f71; font-weight: bold;">1</span>:<span style="color: #8f3f71; font-weight: bold;">3</span>]  <span style="color: #a89984;"># </span><span style="color: #a89984;">indices with max correlation</span>
    <span style="color: #9d0006;">for</span> col <span style="color: #9d0006;">in</span> eachcol(C)
]
<span style="color: #a89984;"># </span><span style="color: #a89984;">e.g. show words for first dictionary element</span>
<span style="color: #076678; font-weight: bold;">@info</span> words[word_indices_per_dict_element[<span style="color: #8f3f71; font-weight: bold;">1</span>]]
</pre>
</div>
<div class="org-src-container">
<pre class="src src-julia">[ Info: [<span style="color: #79740e;">"boat"</span>, <span style="color: #79740e;">"ship"</span>, <span style="color: #79740e;">"sail"</span>]
</pre>
</div>


<p>
Using this strategy also gives us a natural way to plot the relations we have found.
Let us remove all correlations in \(C\) that are not in the top-3 column-wise.
Then, we immediately have an adjacency matrix of a bipartite graph, where the one set of nodes are the words, and the other are the dictionary elements.
We can now use classical graph layouting techniques to draw the resulting connections, e.g. through modeling connections as forces, or using spectral approaches.
</p>


<div id="org281b50c" class="figure">
<p><img src="./figs/graph_graph_from_above.png" alt="graph_graph_from_above.png" width="600" />
</p>
<p><span class="figure-number">Figure 3: </span>The resulting graph with a spring layout. The words around the outside are all words that don&rsquo;t have any edges.</p>
</div>

<div id="orga9c882f" class="figure">
<p><img src="./figs/graph_road_street_walk.png" alt="graph_road_street_walk.png" width="600" />
</p>
<p><span class="figure-number">Figure 4: </span>A zoomed in view of the figure above. A single dictionary element is associated with the words &ldquo;road&rdquo;, &ldquo;street&rdquo;, and &ldquo;walk&rdquo;, with correlations \(\approx0.033\) each.</p>
</div>

<p>
In the future we would like to pull up images associated with a dictionary vector.
</p>
</div>
<div id="outline-container-orga143333" class="outline-4">
<h4 id="orga143333"><span class="section-number-4">3.1.1.</span> Limitations:</h4>
<div class="outline-text-4" id="text-3-1-1">
<ul class="org-ul">
<li>currently no proper scoring method</li>
<li>correlations often low (\(< 0.1\))</li>
<li>currently &ldquo;air&rdquo; and &ldquo;hair&rdquo; etc are assigned to the same caption.</li>
<li>currently only nouns, problem with verbs/adjectives is that they are commonly conjugated.</li>
<li>not clear whether</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgee9fae0" class="outline-3">
<h3 id="orgee9fae0"><span class="section-number-3">3.2.</span> Minimizing semantic distance between image caption and list of descriptive tokens.</h3>
<div class="outline-text-3" id="text-3-2">
<p>
The previous approach was free of any deep learning.
Generating labels through simply checking word \(\in\) caption can be very noisy, both due to not modeling any grammar (e.g. declined nouns, etc), and not capturing any semantics.
</p>

<p>
However, the recent emergence of NLP modeling gives us more tools to deal with the semantic meaning of the captions.
In particular, we can try to generate a &ldquo;bullet point description&rdquo; from the ViT embeddings and then minimize the semantic distance of this description to the caption in an embedded space.
</p>

<p>
More precisely, consider for now the case that each dictionary element has an assigned word token.
Further, consider a model that computes an embedding for any text input (a relatively common type of LLM).
This gives us a simple way to score any mapping from dictionary elements to tokens, and the token assignment problem reduces to a search problem.
</p>

<p>
We propose two ways to deal with this search problem &#x2013; one directly trying to find tokens in the token embedding space, and one using greedy search.
</p>
</div>
<div id="outline-container-orgf0ffef0" class="outline-4">
<h4 id="orgf0ffef0"><span class="section-number-4">3.2.1.</span> Finding tokens by optimizing in the token embedding space.</h4>
<div class="outline-text-4" id="text-3-2-1">
<p>
Given a gradient, we would like to use it to find tokens to assign to dictionary elements.
This method lets us use the gradient by chooising dictionary embeddings directly in the space of token embeddings and using sparse assignments to choose subsets of active dictionary embeddings.
</p>

<p>
Let us denote the text encoding model as \(\mathrm{textenc}\), which takes a sequence of token embeddings, and produces an embedding, i.e. \(\mathrm{textenc}: {Sequence}(\mathbb{R}^e) \rightarrow \mathbb{R}^e\).
(The token embeddings are typically stored as a lookup table with random initialization, although they can also be the result of optimization).
</p>

<p>
We can now try to assign tokens for each dictionary element as follows:
</p>
<ol class="org-ol">
<li>We create a new matrix of randomized dictionary embeddings, i.e. a random vector for each dictionary element.</li>
<li><p>
Then, we create a placeholder prompt, e.g.
</p>
<div class="org-src-container">
<pre class="src src-nil">    An image of an object characterized by:
    - [MASK]
    - [MASK]
    - [MASK]
    ...
</pre>
</div>
<p>
This prompt is encoded into a sequence of embeddings (as is typical).
</p></li>
<li>Now we consider an input image-caption tuple, and the corresponding sparse dictionary assignments for the image given by \(X\).
Using these assignments, we pick the corresponding embeddings from the matrix from step 1, and insert them into the prompt embedding sequence in step 2, essentially replacing the mask tokens by our own token.</li>
<li>Then we process the new sequence of token embeddings with \(\mathrm{textenc}\), and compare the result with calling \(\mathrm{textenc}\) on the caption directly.
We can compute e.g. a MSE score, and use the gradient to update the active token embeddings from step 1.</li>
</ol>


<div id="orge00470f" class="figure">
<p><img src="./tikz-pictures/token-optimization-1/main.png" alt="main.png" width="600" />
</p>
<p><span class="figure-number">Figure 5: </span>An overview over the token optimization in embedding space.</p>
</div>

<p>
The goal here is that we can use the gradient to directly optimize the dictionary embeddings in the token embedding space.
Upon convergence, we would extract the natural language token from the dictionary embeddings by finding the nearest neighbor token embedding and using that token.
</p>

<p>
However, it turns out that in our experiments this approach didn&rsquo;t seem to converge at all.
We believe this is because models are not built to be &ldquo;robust&rdquo; to changes in the token embeddings at all, since these are typically fixed.
Therefore, the gradient information for the dictionary embeddings might be very poorly behaved.
</p>

<p>
For this reason we turn to a more basic approach in the next section.
</p>
</div>
</div>
<div id="outline-container-orgda42e38" class="outline-4">
<h4 id="orgda42e38"><span class="section-number-4">3.2.2.</span> Finding token embeddings through greedy search.</h4>
<div class="outline-text-4" id="text-3-2-2">
<p>
In this setup we adapt a similar approach to the previous section, however we do not rely on the gradient for optimization.
Instead, we simply try to directly set the dictionary embeddings to token embeddings.
</p>

<p>
The algorithm works as follows:
</p>
<ol class="org-ol">
<li>Start by reducing the sparse assignments only to the &ldquo;maximum&rdquo; assignment, and use a prompt with only one mask token.</li>
<li>Then, for every image-caption-sparse-assignments triplet, try out every token the tokenizer supports, and use its embedding to replace the mask token&rsquo;s embedding.</li>
<li>Keep the token that minimizes the mean-squared error (MSE) between the caption encoding and the modified prompt encoding computed over the entire dataset.</li>
<li>Add a second sparse-assignment and mask token, and repeat as above.
Continue until every sparse assignment is processed.</li>
</ol>

<p>
We have had some success with this, but not processed properly yet.
</p>
</div>
</div>
</div>
<div id="outline-container-org985373d" class="outline-3">
<h3 id="org985373d"><span class="section-number-3">3.3.</span> Dictionary learning on a combined (caption-embedding \(\cup\) image-embedding) dataset.</h3>
<div class="outline-text-3" id="text-3-3">
<p>
Another way of mapping dictionary elements to natural language is by computing the k-SVD on a <i>joint</i> dataset that includes both image- and caption-embeddings. (The two embedding models are assumed to be trained jointly e.g. in a CLIP fashion.)
</p>

<p>
Then, each dictionary vector can be mapped back to captions, which can be analyzed for their shared &ldquo;concepts&rdquo;.
</p>

<p>
This approach has not been implemented yet, but it seems like a promising direction that is easy to implement.
</p>
</div>
</div>
</div>
<div id="outline-container-org7dc4517" class="outline-2">
<h2 id="org7dc4517"><span class="section-number-2">4.</span> Experimental results</h2>
<div class="outline-text-2" id="text-4">
<p>
We have many ad-hoc plots, e.g. for the quality of the k-SVD, results from the various token-mappings, but they&rsquo;re not yet cleaned up to put here, and I need to regenerate some datasets (due to a race condition in the data processing) before I can trust the results&#x2026;
Stay tuned!
</p>
</div>
</div>
<div id="outline-container-org9a10f53" class="outline-2">
<h2 id="org9a10f53"><span class="section-number-2">5.</span> Conclusion and current limitations.</h2>
<div class="outline-text-2" id="text-5">
<p>
The current setup assumes that the sparsity constraint is enough to produce dictionary vectors that only need to be aligned downstream.
However, it&rsquo;s not clear that this is true.
Indeed, it&rsquo;s quite likely that performance would be improved if the token mapping could somehow be included in the disentangling step.
This could perhaps be done by propagating a gradient back from the LLM setup and using it in the dictionary learning, which would perhaps integrate better with the Sparse Autoencoder.
However, this is currently out of scope, but would certainly make a good next project.
</p>
</div>
</div>
<div id="outline-container-org696ba02" class="outline-2">
<h2 id="org696ba02"><span class="section-number-2">6.</span> Bibliography.</h2>
<div class="outline-text-2" id="text-6">
<style>.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}</style><div class="csl-bib-body">
  <div class="csl-entry"><a id="citeproc_bib_item_1"></a>Aharon, M., M. Elad, and A. Bruckstein. 2006. “K-SVD: An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation.” <i>Ieee Transactions on Signal Processing</i>, no. 11 (November).</div>
  <div class="csl-entry"><a id="citeproc_bib_item_2"></a>Bezanson, Jeff, Alan Edelman, Stefan Karpinski, and Viral B. Shah. 2017. “Julia: A Fresh Approach to Numerical Computing.” <i>Siam Review</i>, no. 1 (January).</div>
  <div class="csl-entry"><a id="citeproc_bib_item_3"></a>Bricken, Trenton, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner, et al. 2023. “Towards Monosemanticity: Decomposing Language Models with Dictionary Learning.” <i>Transformer Circuits Thread</i>.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_4"></a>Higgins, I., L. Matthey, Arka Pal, Christopher P. Burgess, Xavier Glorot, M. Botvinick, S. Mohamed, and Alexander Lerchner. 2017. “Beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework.” In <i>ICLR</i>.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_5"></a>Locatello, Francesco, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard Schölkopf, and Olivier Bachem. 2019. “Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations.” In <i>Proceedings of the 36th International Conference on Machine Learning</i>. PMLR.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_6"></a>Ordonez, Vicente, Girish Kulkarni, and Tamara Berg. 2011. “Im2Text: Describing Images Using 1 Million Captioned Photographs.” In <i>Advances in Neural Information Processing Systems</i>. Curran Associates, Inc.</div>
</div>
</div>
</div>
<div id="outline-container-orgbeb0049" class="outline-2">
<h2 id="orgbeb0049"><span class="section-number-2">7.</span> Appendix</h2>
<div class="outline-text-2" id="text-7">
</div>
<div id="outline-container-org6d57154" class="outline-3">
<h3 id="org6d57154"><span class="section-number-3">7.1.</span> Correlation patterns <a id="org92f9ae8"></a></h3>
<div class="outline-text-3" id="text-7-1">
<p>
When computing the correlations between word occurrences and dictionary assignments as described in <a href="#org09f021c">3.1</a>, it is interesting to see if dictionary elements correlate with only few, or many words.
Typically we find that for any dictionary element a small number of words clearly stand out by having a higher correlation than the others.
Histrograms showing this are depicted below.
However, we also note that the correlation is typically still low, e.g. \(< 0.1\) for many of the highest-correlating words.
</p>

<div class="org-src-container">
<label class="org-src-name"><span class="listing-number">Listing 2: </span>Word correlations of dictionary element 1.</label><pre class="src src-nil">                ┌                                        ┐
 [-0.02, -0.01) ┤▎ 3
 [-0.01,  0.0 ) ┤█████████████████████████████████  1 017
 [ 0.0 ,  0.01) ┤████████████████▏ 494
 [ 0.01,  0.02) ┤▎ 7
 [ 0.02,  0.03) ┤▏ 1
 [ 0.03,  0.04) ┤▏ 1
 [ 0.04,  0.05) ┤  0
 [ 0.05,  0.06) ┤  0
 [ 0.06,  0.07) ┤▏ 1
 [ 0.07,  0.08) ┤  0
 [ 0.08,  0.09) ┤  0
 [ 0.09,  0.1 ) ┤▏ 1
                └                                        ┘
                                 Frequency
</pre>
</div>

<div class="org-src-container">
<label class="org-src-name"><span class="listing-number">Listing 3: </span>Word correlations of dictionary element 2.</label><pre class="src src-nil">                  ┌                                        ┐
 [-0.005,  0.0  ) ┤█████████████████████████████████  1 265
 [ 0.0  ,  0.005) ┤██████▎ 234
 [ 0.005,  0.01 ) ┤▌ 19
 [ 0.01 ,  0.015) ┤▎ 4
 [ 0.015,  0.02 ) ┤▏ 2
 [ 0.02 ,  0.025) ┤  0
 [ 0.025,  0.03 ) ┤  0
 [ 0.03 ,  0.035) ┤▏ 1
                  └                                        ┘
                                   Frequency
</pre>
</div>

<div class="org-src-container">
<label class="org-src-name"><span class="listing-number">Listing 4: </span>Word correlations of dictionary element 3.</label><pre class="src src-nil">                  ┌                                        ┐
 [-0.01 , -0.005) ┤▎ 7
 [-0.005,  0.0  ) ┤█████████████████████████████████  1 127
 [ 0.0  ,  0.005) ┤██████████▏ 343
 [ 0.005,  0.01 ) ┤▊ 29
 [ 0.01 ,  0.015) ┤▍ 11
 [ 0.015,  0.02 ) ┤▎ 5
 [ 0.02 ,  0.025) ┤▏ 1
 [ 0.025,  0.03 ) ┤▏ 1
 [ 0.03 ,  0.035) ┤  0
 [ 0.035,  0.04 ) ┤▏ 1
                  └                                        ┘
                                   Frequency
</pre>
</div>
</div>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1" role="doc-backlink">1</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
Note that having a linear dependence of \(Y\) wrt \(D\) is a design choice, and one may also consider more nonlinear relations. However, for tractability we restrict ourselves to the linear setting.
</p></div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2" role="doc-backlink">2</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
Alternatively, we can specify a maximum number of dictionary elements per data sample.
</p></div></div>

<div class="footdef"><sup><a id="fn.3" class="footnum" href="#fnr.3" role="doc-backlink">3</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
Unlike the previous two numbers, this hasn&rsquo;t been properly tested yet.
</p></div></div>

<div class="footdef"><sup><a id="fn.4" class="footnum" href="#fnr.4" role="doc-backlink">4</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
A more detailed study on this is to follow.
</p></div></div>


</div>
</div></div>
<div id="postamble" class="status">
<p class="author">Author: Romeo Valentin</p>
<p class="date">Created: 2024-01-14 Sun 17:35</p>
</div>
</body>
</html>
